#!/usr/bin/env python3
"""
W&B Sweepè®­ç»ƒè„šæœ¬ - Transformerè¶…å‚æ•°ä¼˜åŒ–
ç›®æ ‡: ACC_10 > 80% (horizon=1)
"""

import wandb
import yaml
import sys
from pathlib import Path

# æ·»åŠ srcåˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from src.config import Config
from src.data_io import load_data
from src.features import prepare_sequences
from src.models.transformer import TransformerForecaster
from src.cv import rolling_origin_split
from src.metrics import eval_metrics
import numpy as np
import logging


def train():
    """è®­ç»ƒå•æ¬¡sweepå®éªŒ"""
    
    # 1. åˆå§‹åŒ–wandb
    run = wandb.init()
    config = wandb.config
    
    # 2. è®¾ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s | %(levelname)s | %(message)s',
        datefmt='%H:%M:%S'
    )
    
    logging.info(f"ğŸ”§ Sweep Run: {run.name}")
    logging.info(f"ğŸ“Š è¶…å‚æ•°é…ç½®:")
    for key, value in config.items():
        logging.info(f"  {key}: {value}")
    
    try:
        # 3. åŠ è½½åŸºç¡€é…ç½®ï¼ˆä½¿ç”¨sweepä¸“ç”¨é…ç½®ï¼‰
        base_config = Config("config_sweep.yaml")
        
        # 4. è¦†ç›–è¶…å‚æ•°ï¼ˆä½¿ç”¨config.configå­—å…¸è®¿é—®ï¼‰
        base_config.config['models']['transformer']['d_model'] = config.d_model
        base_config.config['models']['transformer']['nhead'] = config.nhead
        base_config.config['models']['transformer']['num_encoder_layers'] = config.num_encoder_layers
        base_config.config['models']['transformer']['num_decoder_layers'] = config.num_decoder_layers
        base_config.config['models']['transformer']['dim_feedforward'] = config.dim_feedforward
        base_config.config['models']['transformer']['dropout'] = config.dropout
        base_config.config['models']['transformer']['learning_rate'] = config.learning_rate
        base_config.config['models']['transformer']['batch_size'] = config.batch_size
        base_config.config['models']['transformer']['epochs'] = config.epochs
        
        base_config.config['features']['sequence_length'] = config.sequence_length
        base_config.config['features']['max_lag'] = config.max_lag
        base_config.config['evaluation']['test_window'] = config.test_window
        
        # 5. åŠ è½½æ•°æ®
        logging.info(f"ğŸ“‚ åŠ è½½æ•°æ® (å¡«å……ç­–ç•¥: {config.strategy})...")
        
        # æ‰¾åˆ°æ•°æ®æ–‡ä»¶
        from pathlib import Path
        import glob
        data_path = Path(base_config.config['data']['data_path'])
        file_pattern = base_config.config['data']['file_pattern']
        data_files = list(data_path.glob(file_pattern))
        
        if not data_files:
            raise FileNotFoundError(f"æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶: {data_path}/{file_pattern}")
        
        data_file = data_files[0]
        logging.info(f"   æ•°æ®æ–‡ä»¶: {data_file}")
        
        # è·å–å¡«å……é…ç½®
        imputation_config = base_config.config.get('data', {}).get('imputation', {})
        
        # è°ƒç”¨load_data
        df_clean, df_before = load_data(
            file_path=str(data_file),
            time_col=base_config.config['data'].get('time_col'),
            p_col=base_config.config['data'].get('p_col'),
            q_col=base_config.config['data'].get('q_col'),
            exog_cols=base_config.config.get('features', {}).get('exog_cols', []),
            freq=base_config.config['data'].get('freq', 'H'),
            tz=base_config.config['data'].get('tz'),
            interp_limit=base_config.config['data'].get('interp_limit', 3),
            imputation_method=config.strategy,
            target_p_value=imputation_config.get('target_p_value', 349),
            day_copy_days_back=imputation_config.get('day_copy_days_back', 7),
            seasonal_period=imputation_config.get('seasonal_period', 24)
        )
        
        logging.info(f"   æ•°æ®å½¢çŠ¶: {df_clean.shape}")
        
        # 6. ç¡®å®šç›®æ ‡åˆ—
        target_cols = []
        predict_p = base_config.config.get('target', {}).get('predict_p', True)
        predict_q = base_config.config.get('target', {}).get('predict_q', False)
        if predict_p:
            target_cols.append('P')
        if predict_q:
            target_cols.append('Q')
        
        logging.info(f"   ç›®æ ‡åˆ—: {target_cols}")
        
        # 7. å‡†å¤‡åºåˆ—æ•°æ®
        logging.info(f"ğŸ”„ å‡†å¤‡åºåˆ—æ•°æ® (horizon={config.horizon}, seq_len={config.sequence_length})...")
        X, Y = prepare_sequences(
            df=df_clean,
            horizon=config.horizon,
            target_cols=target_cols,
            exog_cols=base_config.config.get('features', {}).get('exog_cols', []),
            sequence_length=config.sequence_length
        )
        
        # 8. æ—¶é—´åºåˆ—åˆ†å‰²
        splits = list(rolling_origin_split(
            n_samples=len(X),
            test_window=config.test_window,
            n_splits=1,
            gap=0
        ))
        train_idx, test_idx = splits[0]
        
        X_train, X_test = X[train_idx], X[test_idx]
        y_train, y_test = Y[train_idx], Y[test_idx]
        
        logging.info(f"ğŸ“Š æ•°æ®é›†åˆ’åˆ†:")
        logging.info(f"  è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
        logging.info(f"  æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")
        
        # 9. è®­ç»ƒæ¨¡å‹
        logging.info(f"ğŸš€ å¼€å§‹è®­ç»ƒTransformer...")
        model = TransformerForecaster(
            d_model=config.d_model,
            nhead=config.nhead,
            num_encoder_layers=config.num_encoder_layers,
            num_decoder_layers=config.num_decoder_layers,
            dim_feedforward=config.dim_feedforward,
            dropout=config.dropout,
            epochs=config.epochs,
            batch_size=config.batch_size,
            learning_rate=config.learning_rate,
            device=base_config.config.get('evaluation', {}).get('device', 'auto'),
            n_horizons=1
        )
        model.fit(X_train, y_train)
        
        # 10. é¢„æµ‹
        logging.info(f"ğŸ”® è¿›è¡Œé¢„æµ‹...")
        y_pred = model.predict(X_test)
        
        # 11. è®¡ç®—æŒ‡æ ‡
        logging.info(f"ğŸ“ˆ è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
        metrics = {}
        for i, target_col in enumerate(target_cols):
            y_true_col = y_test[:, i]
            y_pred_col = y_pred[:, i]
            
            col_metrics = eval_metrics(
                y_true=y_true_col,
                y_pred=y_pred_col,
                metric_names=['RMSE', 'MAE', 'SMAPE', 'WAPE', 'ACC_5', 'ACC_10']
            )
            
            # æ·»åŠ å‰ç¼€
            for metric_name, metric_value in col_metrics.items():
                metrics[f"{target_col}_{metric_name}"] = metric_value
        
        # 12. è®°å½•æŒ‡æ ‡åˆ°wandb
        rmse = metrics.get('P_RMSE', 999)
        mae = metrics.get('P_MAE', 999)
        acc_10 = metrics.get('P_ACC_10', 0)
        acc_5 = metrics.get('P_ACC_5', 0)
        
        wandb.log({
            'rmse': rmse,
            'mae': mae,
            'acc_10': acc_10,
            'acc_5': acc_5,
            'smape': metrics.get('P_SMAPE', 999),
            'wape': metrics.get('P_WAPE', 999),
        })
        
        logging.info(f"âœ… è®­ç»ƒå®Œæˆ!")
        logging.info(f"ğŸ“Š æœ€ç»ˆæŒ‡æ ‡:")
        logging.info(f"  RMSE: {rmse:.2f}")
        logging.info(f"  MAE: {mae:.2f}")
        logging.info(f"  ACC_10: {acc_10:.2f}%")
        logging.info(f"  ACC_5: {acc_5:.2f}%")
        
        # åˆ¤æ–­æ˜¯å¦è¾¾æ ‡
        if acc_10 >= 80:
            logging.info(f"ğŸ‰ è¾¾æ ‡! ACC_10 = {acc_10:.2f}% >= 80%")
        else:
            logging.info(f"âš ï¸  æœªè¾¾æ ‡: ACC_10 = {acc_10:.2f}% < 80%")
        
    except Exception as e:
        logging.error(f"âŒ è®­ç»ƒå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        
        # è®°å½•å¤±è´¥
        wandb.log({
            'rmse': 999,
            'mae': 999,
            'acc_10': 0,
            'acc_5': 0,
        })
        raise
    
    finally:
        wandb.finish()


if __name__ == "__main__":
    train()
