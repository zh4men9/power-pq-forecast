"""
è®¾å¤‡æ€§èƒ½åŸºå‡†æµ‹è¯•ï¼šå¯¹æ¯” CPU vs MPS
æµ‹è¯• LSTM å’Œ Transformer åœ¨ä¸åŒè®¾å¤‡ä¸Šçš„è®­ç»ƒé€Ÿåº¦
"""

import torch
import torch.nn as nn
import numpy as np
import time
from pathlib import Path
import pandas as pd

# å¯¼å…¥æ¨¡å‹
from src.models.lstm import LSTMForecaster
from src.models.transformer import TransformerForecaster

def create_dummy_data(n_samples=3000, seq_length=24, n_features=9, batch_size=64):
    """åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®"""
    X = torch.randn(n_samples, seq_length, n_features)
    y = torch.randn(n_samples, 1)
    
    # åˆ›å»º DataLoader
    dataset = torch.utils.data.TensorDataset(X, y)
    loader = torch.utils.data.DataLoader(
        dataset, 
        batch_size=batch_size, 
        shuffle=True
    )
    return loader

def benchmark_model(model_type, device, n_epochs=1, batch_size=64, **model_kwargs):
    """
    åŸºå‡†æµ‹è¯•å•ä¸ªæ¨¡å‹
    
    Args:
        model_type: 'lstm' æˆ– 'transformer'
        device: 'cpu', 'mps', æˆ– 'cuda'
        n_epochs: è®­ç»ƒè½®æ•°
        batch_size: æ‰¹æ¬¡å¤§å°
        **model_kwargs: æ¨¡å‹å‚æ•°
    
    Returns:
        è®­ç»ƒæ—¶é—´ï¼ˆç§’ï¼‰
    """
    # åˆ›å»ºæ•°æ®
    loader = create_dummy_data(batch_size=batch_size)
    
    # åˆ›å»ºæ¨¡å‹ï¼ˆç›´æ¥ç”¨PyTorchçš„åº•å±‚æ¨¡å‹ï¼‰
    if model_type == 'lstm':
        from src.models.lstm import LSTMModel
        model = LSTMModel(
            input_size=9,
            hidden_size=model_kwargs.get('hidden_size', 64),
            num_layers=model_kwargs.get('num_layers', 2),
            output_size=1,
            dropout=model_kwargs.get('dropout', 0.2)
        )
    else:  # transformer
        from src.models.transformer import TransformerModel
        model = TransformerModel(
            input_size=9,
            d_model=model_kwargs.get('d_model', 64),
            nhead=model_kwargs.get('nhead', 4),
            num_encoder_layers=model_kwargs.get('num_encoder_layers', 2),
            num_decoder_layers=model_kwargs.get('num_decoder_layers', 2),
            dim_feedforward=model_kwargs.get('dim_feedforward', 256),
            dropout=model_kwargs.get('dropout', 0.1),
            output_size=1
        )
    
    model = model.to(device)
    
    # ä¼˜åŒ–å™¨å’ŒæŸå¤±
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.MSELoss()
    
    # é¢„çƒ­ï¼ˆé¿å…é¦–æ¬¡è¿è¡Œçš„åˆå§‹åŒ–å¼€é”€ï¼‰
    model.train()
    for X_batch, y_batch in loader:
        X_batch = X_batch.to(device)
        y_batch = y_batch.to(device)
        
        optimizer.zero_grad()
        y_pred = model(X_batch)
        loss = criterion(y_pred, y_batch)
        loss.backward()
        optimizer.step()
        break  # åªè·‘ä¸€ä¸ªbatché¢„çƒ­
    
    # æ­£å¼è®¡æ—¶
    start_time = time.time()
    
    for epoch in range(n_epochs):
        model.train()
        epoch_loss = 0.0
        n_batches = 0
        
        for X_batch, y_batch in loader:
            X_batch = X_batch.to(device)
            y_batch = y_batch.to(device)
            
            optimizer.zero_grad()
            y_pred = model(X_batch)
            loss = criterion(y_pred, y_batch)
            loss.backward()
            optimizer.step()
            
            epoch_loss += loss.item()
            n_batches += 1
        
        avg_loss = epoch_loss / n_batches
        print(f"  Epoch {epoch+1}/{n_epochs}, Loss: {avg_loss:.4f}")
    
    elapsed_time = time.time() - start_time
    
    return elapsed_time

def main():
    print("="*70)
    print("è®¾å¤‡æ€§èƒ½åŸºå‡†æµ‹è¯•ï¼šCPU vs MPS")
    print("="*70)
    
    # æ£€æŸ¥å¯ç”¨è®¾å¤‡
    print("\nğŸ“± å¯ç”¨è®¾å¤‡:")
    print(f"  - CPU: âœ“")
    print(f"  - MPS: {'âœ“' if torch.backends.mps.is_available() else 'âœ—'}")
    print(f"  - CUDA: {'âœ“' if torch.cuda.is_available() else 'âœ—'}")
    
    devices = ['cpu']
    if torch.backends.mps.is_available():
        devices.append('mps')
    if torch.cuda.is_available():
        devices.append('cuda')
    
    # æµ‹è¯•é…ç½®
    configs = [
        {
            'name': 'LSTM (batch=32)',
            'model_type': 'lstm',
            'batch_size': 32,
            'model_kwargs': {
                'hidden_size': 64,
                'num_layers': 2,
                'dropout': 0.3
            }
        },
        {
            'name': 'LSTM (batch=64)',
            'model_type': 'lstm',
            'batch_size': 64,
            'model_kwargs': {
                'hidden_size': 64,
                'num_layers': 2,
                'dropout': 0.3
            }
        },
        {
            'name': 'Transformer (batch=32)',
            'model_type': 'transformer',
            'batch_size': 32,
            'model_kwargs': {
                'd_model': 64,
                'nhead': 4,
                'num_encoder_layers': 2,
                'num_decoder_layers': 2,
                'dim_feedforward': 256,
                'dropout': 0.2
            }
        },
        {
            'name': 'Transformer (batch=64)',
            'model_type': 'transformer',
            'batch_size': 64,
            'model_kwargs': {
                'd_model': 64,
                'nhead': 4,
                'num_encoder_layers': 2,
                'num_decoder_layers': 2,
                'dim_feedforward': 256,
                'dropout': 0.2
            }
        }
    ]
    
    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    results = []
    
    for config in configs:
        print("\n" + "="*70)
        print(f"ğŸ§ª æµ‹è¯•: {config['name']}")
        print("="*70)
        
        for device in devices:
            print(f"\nâ–¶ è®¾å¤‡: {device.upper()}")
            try:
                elapsed = benchmark_model(
                    model_type=config['model_type'],
                    device=device,
                    n_epochs=1,
                    batch_size=config['batch_size'],
                    **config['model_kwargs']
                )
                print(f"  âœ“ å®Œæˆæ—¶é—´: {elapsed:.2f} ç§’")
                
                results.append({
                    'Model': config['name'],
                    'Device': device.upper(),
                    'Time (s)': elapsed,
                    'Batch Size': config['batch_size']
                })
            except Exception as e:
                print(f"  âœ— å¤±è´¥: {e}")
    
    # ç”ŸæˆæŠ¥å‘Š
    print("\n" + "="*70)
    print("ğŸ“Š åŸºå‡†æµ‹è¯•ç»“æœæ±‡æ€»")
    print("="*70)
    
    df = pd.DataFrame(results)
    
    # æŒ‰æ¨¡å‹åˆ†ç»„æ˜¾ç¤º
    for model_name in df['Model'].unique():
        model_df = df[df['Model'] == model_name]
        print(f"\n{model_name}:")
        print("-" * 50)
        
        for device in devices:
            device_data = model_df[model_df['Device'] == device.upper()]
            if not device_data.empty:
                time_val = device_data['Time (s)'].values[0]
                print(f"  {device.upper():8s}: {time_val:6.2f}ç§’")
        
        # è®¡ç®—åŠ é€Ÿæ¯”
        if len(devices) > 1:
            cpu_time = model_df[model_df['Device'] == 'CPU']['Time (s)'].values
            if 'mps' in devices:
                mps_data = model_df[model_df['Device'] == 'MPS']
                if not mps_data.empty and len(cpu_time) > 0:
                    mps_time = mps_data['Time (s)'].values[0]
                    speedup = cpu_time[0] / mps_time
                    if speedup > 1:
                        print(f"  ğŸ’¡ MPS åŠ é€Ÿ: {speedup:.2f}x å¿«äºCPU")
                    else:
                        print(f"  âš ï¸  MPS åè€Œæ…¢: CPUå¿« {1/speedup:.2f}x")
    
    # ä¿å­˜ç»“æœ
    output_file = 'benchmark_results.csv'
    df.to_csv(output_file, index=False)
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜è‡³: {output_file}")
    
    # ç»“è®º
    print("\n" + "="*70)
    print("ğŸ’¡ ç»“è®ºä¸å»ºè®®")
    print("="*70)
    
    if 'mps' in devices:
        cpu_avg = df[df['Device'] == 'CPU']['Time (s)'].mean()
        mps_avg = df[df['Device'] == 'MPS']['Time (s)'].mean()
        
        if mps_avg < cpu_avg:
            speedup = cpu_avg / mps_avg
            print(f"âœ… MPS å¹³å‡å¿« {speedup:.2f}xï¼Œå»ºè®®ä½¿ç”¨ MPS")
        else:
            slowdown = mps_avg / cpu_avg
            print(f"âš ï¸  MPS å¹³å‡æ…¢ {slowdown:.2f}xï¼Œå»ºè®®ä½¿ç”¨ CPU")
            print("\nå¯èƒ½åŸå› :")
            print("  1. Batch size å¤ªå° (< 64)ï¼ŒGPUå¯åŠ¨å¼€é”€å¤§äºè®¡ç®—æ”¶ç›Š")
            print("  2. æ¨¡å‹å¤ªå°ï¼ŒCPUå·²ç»å¾ˆå¿«")
            print("  3. æ•°æ®ä¼ è¾“å¼€é”€ (CPU â†” MPS) å æ¯”å¤§")
            print("\nå»ºè®®:")
            print("  - å¢å¤§ batch_size åˆ° 128-256")
            print("  - æˆ–è€…ç›´æ¥ç”¨ CPUï¼ˆå¯¹å°æ¨¡å‹æ›´é«˜æ•ˆï¼‰")

if __name__ == '__main__':
    main()
